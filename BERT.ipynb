{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4558ceea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.46.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: accelerate in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (1.11.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers>=4.46.0) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers>=4.46.0) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers>=4.46.0) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers>=4.46.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers>=4.46.0) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers>=4.46.0) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers>=4.46.0) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers>=4.46.0) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers>=4.46.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from transformers>=4.46.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46.0) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from accelerate) (2.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from requests->transformers>=4.46.0) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from requests->transformers>=4.46.0) (2.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers>=4.46.0) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mikha\\onedrive\\desktop\\nlp\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U \"transformers>=4.46.0\" datasets accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851684f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, torch\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ecc1af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repro\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED); np.random.seed(SEED)\n",
    "\n",
    "# Model: keep BERT; if your laptop wheezes, swap to \"distilbert-base-uncased\"\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# File paths (put your CSVs in the same folder)\n",
    "TRAIN_PATH = \"train.csv\"\n",
    "TEST_PATH  = \"test.csv\"\n",
    "\n",
    "# Labels in your preprocessed data\n",
    "LABELS   = [\"concur\",\"oppose\",\"neutral\"]\n",
    "LABEL2ID = {l:i for i,l in enumerate(LABELS)}\n",
    "ID2LABEL = {i:l for l,i in LABEL2ID.items()}\n",
    "\n",
    "# CPU-friendly defaults\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "BATCH_SIZE = 4      \n",
    "MAX_LEN = 128  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d691dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4504\n",
      "stance\n",
      "concur     4049\n",
      "oppose      447\n",
      "neutral       8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def load_split(path: str):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(path, encoding=\"latin1\")\n",
    "\n",
    "    need = {\"post_text\",\"comment_text\"}  # stance optional for pure inference\n",
    "    assert need.issubset(df.columns), f\"{path} must have columns: {need}\"\n",
    "\n",
    "    # hygiene\n",
    "    df[\"post_text\"]    = df[\"post_text\"].astype(str).str.strip()\n",
    "    df[\"comment_text\"] = df[\"comment_text\"].astype(str).str.strip()\n",
    "\n",
    "    if \"stance\" in df.columns:\n",
    "        df[\"stance\"] = df[\"stance\"].astype(str).str.lower().str.strip()\n",
    "        df = df[df[\"stance\"].isin(LABEL2ID)]               # keep only known labels\n",
    "        df[\"label\"] = df[\"stance\"].map(LABEL2ID).astype(int)\n",
    "\n",
    "    # no duplicate triples\n",
    "    keep_cols = [\"post_text\",\"comment_text\"] + ([\"label\"] if \"label\" in df.columns else [])\n",
    "    df = df.drop_duplicates(subset=keep_cols).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df_reddit = load_split(r\"C:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\reddit_posts_and_comments_train.csv\")\n",
    "test_df_reddit = load_split(r\"C:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\reddit_posts_and_comments_test.csv\")\n",
    "\n",
    "print(\"Train size:\", len(train_df_reddit))\n",
    "if \"stance\" in train_df_reddit.columns:\n",
    "    print(train_df_reddit[\"stance\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f871bd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1136 entries, 0 to 1135\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   post_text     1136 non-null   object\n",
      " 1   comment_text  1136 non-null   object\n",
      " 2   stance        1136 non-null   object\n",
      " 3   label         1136 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 35.6+ KB\n"
     ]
    }
   ],
   "source": [
    "test_df_reddit.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af27b77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (3603, 4)\n",
      "Val shape: (901, 4)\n",
      "Label distribution in train:\n",
      " label\n",
      "0    3239\n",
      "1     358\n",
      "2       6\n",
      "Name: count, dtype: int64\n",
      "Label distribution in val:\n",
      " label\n",
      "0    810\n",
      "1     89\n",
      "2      2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df = train_test_split(train_df_reddit, test_size=0.2, random_state=42, stratify=train_df_reddit['stance'])\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Val shape:\", val_df.shape)\n",
    "print(\"Label distribution in train:\\n\", train_df[\"label\"].value_counts())\n",
    "print(\"Label distribution in val:\\n\", val_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74eff69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3603/3603 [00:06<00:00, 569.35 examples/s]\n",
      "Map: 100%|██████████| 901/901 [00:11<00:00, 77.92 examples/s]\n",
      "Map: 100%|██████████| 1136/1136 [00:01<00:00, 819.21 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "def make_ds(df: pd.DataFrame, has_labels: bool):\n",
    "    cols = [\"comment_text\",\"post_text\"] + ([\"label\"] if has_labels else [])\n",
    "    ds = Dataset.from_pandas(df[cols])\n",
    "\n",
    "    def _tok(batch):\n",
    "        return tokenizer(\n",
    "            batch[\"comment_text\"],   # text A (comment)\n",
    "            batch[\"post_text\"],      # text B (post)\n",
    "            padding =True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "\n",
    "    ds = ds.map(_tok, batched=True, remove_columns=[\"comment_text\",\"post_text\"])\n",
    "    if has_labels:\n",
    "        ds = ds.with_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"token_type_ids\",\"label\"])\n",
    "    else:\n",
    "        ds = ds.with_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"token_type_ids\"])\n",
    "    return ds\n",
    "\n",
    "#train_ds_reddit = make_ds(train_df_reddit, has_labels=True)\n",
    "#test_ds_reddit  = make_ds(test_df_reddit,  has_labels=(\"label\" in test_df_reddit.columns))\n",
    "train_ds = make_ds(train_df, has_labels=True)\n",
    "val_ds = make_ds(val_df, has_labels=True)\n",
    "test_ds  = make_ds(test_df_reddit,  has_labels=(\"label\" in test_df_reddit.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b965af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    # Precision / Recall / F1 (macro and micro)\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    precision_micro, recall_micro, f1_micro, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average=\"micro\", zero_division=0\n",
    "    )\n",
    "\n",
    "    # Specificity (average true negative rate)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    num_classes = cm.shape[0]\n",
    "    specificity_scores = []\n",
    "    for i in range(num_classes):\n",
    "        tn = np.sum(np.delete(np.delete(cm, i, axis=0), i, axis=1))  # remove row/col i\n",
    "        fp = np.sum(cm[:, i]) - cm[i, i]\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        specificity_scores.append(specificity)\n",
    "    specificity_macro = np.mean(specificity_scores)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_macro\": precision_macro,\n",
    "        \"recall_macro\": recall_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_micro\": precision_micro,\n",
    "        \"recall_micro\": recall_micro,\n",
    "        \"f1_micro\": f1_micro,\n",
    "        \"specificity_macro\": specificity_macro\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb2e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000028A06E63250>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b22fd5a9-9887-47c8-bad3-5cb1a705abfd)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000028A06E847D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 78b75179-0c40-42b7-90ec-7bf9c32de1f7)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /bert-base-uncased/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000028A06E85110>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 4f9a5c60-b4a5-4bc5-a96c-c3494d6525d3)')' thrown while requesting HEAD https://huggingface.co/bert-base-uncased/resolve/main/config.json\n",
      "Retrying in 4s [Retry 3/5].\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\1802429826.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=len(LABELS),\n",
    "    id2label=ID2LABEL,\n",
    "    label2id=LABEL2ID\n",
    ")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./bert_stance_baseline\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    eval_strategy=\"epoch\",        # use correct arg name\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    greater_is_better=True,\n",
    "    report_to=[]                         # avoid wandb nagging\n",
    ")\n",
    "    # keep it minimal so it works across transformers versions\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660e7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 22:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.520400</td>\n",
       "      <td>0.546900</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.392200</td>\n",
       "      <td>0.580518</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.388500</td>\n",
       "      <td>0.565612</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5469000935554504, 'eval_accuracy': 0.88, 'eval_precision_macro': 0.29333333333333333, 'eval_recall_macro': 0.3333333333333333, 'eval_f1_macro': 0.3120567375886525, 'eval_precision_micro': 0.88, 'eval_recall_micro': 0.88, 'eval_f1_micro': 0.88, 'eval_specificity_macro': 0.6666666666666666, 'eval_runtime': 10.4524, 'eval_samples_per_second': 4.784, 'eval_steps_per_second': 1.244, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      concur       0.88      1.00      0.94        44\n",
      "      oppose       0.00      0.00      0.00         5\n",
      "     neutral       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.88        50\n",
      "   macro avg       0.29      0.33      0.31        50\n",
      "weighted avg       0.77      0.88      0.82        50\n",
      "\n",
      "[[44  0  0]\n",
      " [ 5  0  0]\n",
      " [ 1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "# Final evaluation on test_ds (same set if you used it as eval)\n",
    "metrics = trainer.evaluate(val_ds)\n",
    "print(metrics)\n",
    "\n",
    "# If you want a confusion matrix and per-class report:\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pred = trainer.predict(val_ds)\n",
    "y_pred = pred.predictions.argmax(axis=1)\n",
    "y_true = np.array(val_ds[\"label\"])\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=LABELS))\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ded60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "def make_trainer_for_fold(args, train_ds, val_ds):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=len(LABELS), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "    )\n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,   ##\n",
    "        eval_dataset=val_ds,        ## \n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer),\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "def objective(trial, full_df, n_splits=5):\n",
    "    # Search space (keep tight on CPU)\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    batch = trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8])\n",
    "    epochs = trial.suggest_int(\"num_train_epochs\", 2, 3)\n",
    "    wd = trial.suggest_float(\"weight_decay\", 0.0, 0.1)\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    f1s = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(skf.split(full_df, full_df[\"label\"]), 1):\n",
    "        tr_df = full_df.iloc[tr_idx].reset_index(drop=True)\n",
    "        va_df = full_df.iloc[va_idx].reset_index(drop=True)\n",
    "        tr_ds = make_ds(tr_df, has_labels=True)\n",
    "        va_ds = make_ds(va_df, has_labels=True)\n",
    "\n",
    "        args = TrainingArguments(\n",
    "            output_dir=f\"./cv_trial{trial.number}_fold{fold}\",\n",
    "            learning_rate=lr,\n",
    "            per_device_train_batch_size=batch,\n",
    "            per_device_eval_batch_size=batch,\n",
    "            num_train_epochs=epochs,\n",
    "            weight_decay=wd,\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"no\",\n",
    "            load_best_model_at_end=False,\n",
    "            dataloader_num_workers=0,\n",
    "            logging_steps=50,\n",
    "            report_to=[],\n",
    "            seed=SEED\n",
    "        )\n",
    "\n",
    "        trainer = make_trainer_for_fold(args, tr_ds, va_ds)\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate(val_ds)\n",
    "        f1s.append(metrics[\"eval_f1_macro\"])\n",
    "\n",
    "        # pruning support\n",
    "        trial.report(metrics[\"eval_f1_macro\"], step=fold)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    mean_f1 = float(np.mean(f1s))\n",
    "    trial.set_user_attr(\"fold_f1s\", f1s)\n",
    "    return mean_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b672a313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 22:06:46,972] A new study created in memory with name: bert_stance_cv\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 200/200 [00:00<00:00, 593.91 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1433.60 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 11:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.510500</td>\n",
       "      <td>0.574434</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.388900</td>\n",
       "      <td>0.588394</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.382500</td>\n",
       "      <td>0.579268</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 274.94 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 599.19 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 15:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.472400</td>\n",
       "      <td>0.558372</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.558284</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.587540</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:01<00:00, 113.11 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 630.15 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 13:56, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.472400</td>\n",
       "      <td>0.558372</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.400400</td>\n",
       "      <td>0.558284</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.587540</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.425877:  17%|█▋        | 1/6 [43:15<3:36:16, 2595.22s/it]c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 22:50:02,367] Trial 0 finished with value: 0.42587719298245613 and parameters: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 4, 'num_train_epochs': 3, 'weight_decay': 0.015601864044243652}. Best is trial 0 with value: 0.42587719298245613.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 669.33 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1236.59 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 17:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424632</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>0.432177</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.459803</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 345.27 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 500.54 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 13:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424632</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>0.432177</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.459803</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 361.73 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 845.77 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 12:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.424632</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.513100</td>\n",
       "      <td>0.432177</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.324900</td>\n",
       "      <td>0.459803</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.425877:  33%|███▎      | 2/6 [1:29:28<3:00:01, 2700.25s/it]c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-10 23:36:16,155] Trial 1 finished with value: 0.42531995346131474 and parameters: {'learning_rate': 1.2853916978930139e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 3, 'weight_decay': 0.07080725777960455}. Best is trial 0 with value: 0.42587719298245613.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 460.29 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 800.23 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 07:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.564700</td>\n",
       "      <td>0.443443</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.343200</td>\n",
       "      <td>0.487808</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 405.25 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 586.62 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 07:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.633300</td>\n",
       "      <td>0.437562</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.326800</td>\n",
       "      <td>0.476265</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 366.52 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1410.02 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 10:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.633300</td>\n",
       "      <td>0.437562</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.326800</td>\n",
       "      <td>0.476265</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.425877:  50%|█████     | 3/6 [1:56:32<1:50:25, 2208.59s/it]c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 00:03:19,640] Trial 2 finished with value: 0.42531995346131474 and parameters: {'learning_rate': 1.0336843570697396e-05, 'per_device_train_batch_size': 4, 'num_train_epochs': 2, 'weight_decay': 0.018182496720710064}. Best is trial 0 with value: 0.42587719298245613.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 302.78 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 571.33 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 09:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.475253</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.569800</td>\n",
       "      <td>0.443171</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 245.12 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 473.20 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 11:15, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.475253</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.569800</td>\n",
       "      <td>0.443171</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 248.56 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1075.07 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 08:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.475253</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.569800</td>\n",
       "      <td>0.443171</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.425877:  67%|██████▋   | 4/6 [2:28:36<1:09:52, 2096.10s/it]c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 00:35:23,327] Trial 3 finished with value: 0.42531995346131474 and parameters: {'learning_rate': 1.34336568680343e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.029122914019804193}. Best is trial 0 with value: 0.42587719298245613.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 301.01 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 604.05 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 14:47, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.448736</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.451700</td>\n",
       "      <td>0.463430</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 282.23 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 453.39 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 12:21, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.448736</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.451700</td>\n",
       "      <td>0.463430</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 280.24 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 556.93 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 15:04, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.448736</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.451700</td>\n",
       "      <td>0.463430</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.425877:  83%|████████▎ | 5/6 [3:13:10<38:24, 2304.50s/it]  c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:811: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 01:19:57,325] Trial 4 finished with value: 0.42531995346131474 and parameters: {'learning_rate': 2.6771137242145903e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.0456069984217036}. Best is trial 0 with value: 0.42587719298245613.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 200/200 [00:00<00:00, 528.13 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 1256.79 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\mikha\\AppData\\Local\\Temp\\ipykernel_22460\\2237501926.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  return Trainer(\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='114' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [114/114 14:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Macro</th>\n",
       "      <th>Recall Macro</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>Precision Micro</th>\n",
       "      <th>Recall Micro</th>\n",
       "      <th>F1 Micro</th>\n",
       "      <th>Specificity Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.470063</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.421000</td>\n",
       "      <td>0.530378</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.306600</td>\n",
       "      <td>0.500538</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.312057</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.880000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mikha\\OneDrive\\Desktop\\NLP\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.425877: 100%|██████████| 6/6 [3:28:01<00:00, 2080.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 01:34:49,010] Trial 5 pruned. \n",
      "Best CV macro-F1: 0.42587719298245613\n",
      "Best params: {'learning_rate': 1.827226177606625e-05, 'per_device_train_batch_size': 4, 'num_train_epochs': 3, 'weight_decay': 0.015601864044243652}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>state</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_num_train_epochs</th>\n",
       "      <th>params_per_device_train_batch_size</th>\n",
       "      <th>params_weight_decay</th>\n",
       "      <th>user_attrs_fold_f1s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.425877</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.015602</td>\n",
       "      <td>[0.4791666666666667, 0.4791666666666667, 0.319...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.425320</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.070807</td>\n",
       "      <td>[0.4791666666666667, 0.4791666666666667, 0.317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.425320</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.018182</td>\n",
       "      <td>[0.4791666666666667, 0.4791666666666667, 0.317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.425320</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.029123</td>\n",
       "      <td>[0.4791666666666667, 0.4791666666666667, 0.317...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.425320</td>\n",
       "      <td>COMPLETE</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.045607</td>\n",
       "      <td>[0.4791666666666667, 0.4791666666666667, 0.317...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number     value     state  params_learning_rate  params_num_train_epochs  \\\n",
       "0       0  0.425877  COMPLETE              0.000018                        3   \n",
       "1       1  0.425320  COMPLETE              0.000013                        3   \n",
       "2       2  0.425320  COMPLETE              0.000010                        2   \n",
       "3       3  0.425320  COMPLETE              0.000013                        2   \n",
       "4       4  0.425320  COMPLETE              0.000027                        2   \n",
       "\n",
       "   params_per_device_train_batch_size  params_weight_decay  \\\n",
       "0                                   4             0.015602   \n",
       "1                                   8             0.070807   \n",
       "2                                   4             0.018182   \n",
       "3                                   8             0.029123   \n",
       "4                                   8             0.045607   \n",
       "\n",
       "                                 user_attrs_fold_f1s  \n",
       "0  [0.4791666666666667, 0.4791666666666667, 0.319...  \n",
       "1  [0.4791666666666667, 0.4791666666666667, 0.317...  \n",
       "2  [0.4791666666666667, 0.4791666666666667, 0.317...  \n",
       "3  [0.4791666666666667, 0.4791666666666667, 0.317...  \n",
       "4  [0.4791666666666667, 0.4791666666666667, 0.317...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure train_df_reddit has labels\n",
    "assert \"label\" in sample_train.columns\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=SEED)\n",
    "pruner  = optuna.pruners.MedianPruner(n_warmup_steps=1)\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler, pruner=pruner, study_name=\"bert_stance_cv\")\n",
    "study.optimize(lambda t: objective(t, train_ds, n_splits=3), n_trials=6, show_progress_bar=True)\n",
    "\n",
    "print(\"Best CV macro-F1:\", study.best_value)\n",
    "print(\"Best params:\", study.best_params)\n",
    "\n",
    "# Keep all trials for later visuals\n",
    "trials_df = study.trials_dataframe(attrs=(\"number\",\"value\",\"state\",\"params\",\"user_attrs\",\"system_attrs\"))\n",
    "trials_df.to_csv(\"optuna_trials.csv\", index=False)\n",
    "trials_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9551dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m best = \u001b[43mstudy\u001b[49m.best_params\n\u001b[32m      3\u001b[39m full_train_ds = make_ds(sample_train, has_labels=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      4\u001b[39m full_test_ds  = make_ds(sample_test,  has_labels=(\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m test_df_reddit.columns))\n",
      "\u001b[31mNameError\u001b[39m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "best = study.best_params\n",
    "\n",
    "full_train_ds = make_ds(train_df_reddit, has_labels=True)\n",
    "full_test_ds  = make_ds(test_df_reddit,  has_labels=(\"label\" in test_df_reddit.columns))\n",
    "\n",
    "final_args = TrainingArguments(\n",
    "    output_dir=\"./final_cv_best\",\n",
    "    learning_rate=best[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best.get(\"num_train_epochs\", EPOCHS),\n",
    "    weight_decay=best[\"weight_decay\"],\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    dataloader_num_workers=0,\n",
    "    logging_steps=50,\n",
    "    report_to=[],\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME, num_labels=len(LABELS), id2label=ID2LABEL, label2id=LABEL2ID\n",
    "    ),\n",
    "    args=final_args,\n",
    "    train_dataset=full_train_ds,\n",
    "    eval_dataset=full_test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "final_trainer.train()\n",
    "final_metrics = final_trainer.evaluate(full_test_ds)\n",
    "print(final_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1a4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.importance import FanovaImportanceEvaluator\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_param_importances, plot_optimization_history, plot_slice,\n",
    "    plot_parallel_coordinate, plot_contour\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Importance (fANOVA)\n",
    "imp = optuna.importance.get_param_importances(study, evaluator=FanovaImportanceEvaluator())\n",
    "print(\"Param importances:\")\n",
    "for k, v in imp.items():\n",
    "    print(f\"{k:28s} {v:.3f}\")\n",
    "\n",
    "fig = plot_param_importances(study); fig.suptitle(\"Hyperparameter Importance\"); fig.savefig(\"param_importances.png\", dpi=200, bbox_inches=\"tight\")\n",
    "fig = plot_optimization_history(study); fig.suptitle(\"Optimization History\"); fig.savefig(\"opt_history.png\", dpi=200, bbox_inches=\"tight\")\n",
    "\n",
    "# 2) Per-parameter response (effect on score across all trials)\n",
    "fig = plot_slice(study); fig.suptitle(\"Per-Parameter Performance Slices\"); fig.savefig(\"param_slices.png\", dpi=200, bbox_inches=\"tight\")\n",
    "\n",
    "# 3) Interactions\n",
    "fig = plot_parallel_coordinate(study); fig.suptitle(\"Parameter Interactions\"); fig.savefig(\"parallel_coords.png\", dpi=200, bbox_inches=\"tight\")\n",
    "fig = plot_contour(study); fig.suptitle(\"Pairwise Contours\"); fig.savefig(\"contours.png\", dpi=200, bbox_inches=\"tight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a830d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials_df already saved above\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "df = trials_df[trials_df[\"state\"]==\"COMPLETE\"].copy()\n",
    "df[\"value\"] = df[\"value\"].astype(float)\n",
    "\n",
    "def plot_numeric_response(df, param, metric_col=\"value\", bins=8):\n",
    "    x = pd.to_numeric(df[f\"params_{param}\"], errors=\"coerce\")\n",
    "    y = df[metric_col]\n",
    "    m = ~x.isna() & ~y.isna()\n",
    "    x, y = x[m].values, y[m].values\n",
    "    if len(x) < 2: return\n",
    "    plt.figure()\n",
    "    plt.scatter(x, y, alpha=0.65)\n",
    "    plt.xlabel(param); plt.ylabel(metric_col); plt.title(f\"{param} vs {metric_col}\")\n",
    "    edges = np.linspace(x.min(), x.max(), bins+1)\n",
    "    idx = np.digitize(x, edges) - 1\n",
    "    means = [y[idx==i].mean() for i in range(bins)]\n",
    "    mids  = (edges[:-1] + edges[1:]) / 2\n",
    "    plt.plot(mids, means, linewidth=2)\n",
    "    plt.tight_layout(); plt.savefig(f\"resp_{param}.png\", dpi=200); plt.close()\n",
    "\n",
    "def plot_categorical_response(df, param, metric_col=\"value\"):\n",
    "    sub = df[[f\"params_{param}\", metric_col]].dropna()\n",
    "    if sub.empty: return\n",
    "    g = sub.groupby(f\"params_{param}\")[metric_col]\n",
    "    cats, means, stds = list(g.mean().index), g.mean().values, g.std().fillna(0).values\n",
    "    plt.figure()\n",
    "    pos = np.arange(len(cats))\n",
    "    plt.bar(pos, means, yerr=stds)\n",
    "    plt.xticks(pos, cats, rotation=20, ha=\"right\")\n",
    "    plt.ylabel(metric_col); plt.title(f\"{param} (mean ± std)\")\n",
    "    plt.tight_layout(); plt.savefig(f\"resp_{param}.png\", dpi=200); plt.close()\n",
    "\n",
    "for c in df.columns:\n",
    "    if c.startswith(\"params_\"):\n",
    "        p = c.replace(\"params_\",\"\")\n",
    "        series = df[c]\n",
    "        # heuristic: if most values parse to numeric, treat as numeric\n",
    "        if pd.to_numeric(series, errors=\"coerce\").notna().mean() > 0.7:\n",
    "            plot_numeric_response(df, p)\n",
    "        else:\n",
    "            plot_categorical_response(df, p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
